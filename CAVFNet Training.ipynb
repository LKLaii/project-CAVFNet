{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecd539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pltm(history,save_dir):\n",
    "    plt.figure(figsize=(16,5), dpi=85)\n",
    "    ax1 = plt.subplot(121)#loss    \n",
    "    plt.plot(history[\"loss\"][\"train\"], label=\"train_loss\")\n",
    "    plt.plot(history[\"loss\"][\"val\"], label=\"val_loss\")\n",
    "    plt.legend()  \n",
    "    plt.xlabel('epoches')\n",
    "    plt.title('Loss')\n",
    "    \n",
    "    ax2 = plt.subplot(122)#acc\n",
    "    plt.plot(history[\"accuracy\"][\"train\"], label=\"train_acc\")\n",
    "    plt.plot(history[\"accuracy\"][\"val\"], label=\"val_acc\")\n",
    "    plt.legend()  \n",
    "    plt.xlabel('epoches')\n",
    "    plt.ylabel('%')\n",
    "    plt.title('Acc')\n",
    "#     plt.show\n",
    "    plt.savefig(os.path.join(save_dir,'train-loss-acc-plt.png'))\n",
    "\n",
    "def train( train_dataset, test_dataset,bestacc, model, max_epoches, learning_rate,batch_size, subdir):\n",
    "    # train model\n",
    "    history = {\"accuracy\": {\"train\": [],\"val\": [] },\n",
    "               \"loss\": { \"train\": [], \"val\": [] },\n",
    "               \"lr\":[learning_rate],\n",
    "               'best_score':{'val':0,'test':0} }\n",
    "    \n",
    "    step_size=100\n",
    "    gamma=0.8\n",
    "    loss_fn =nn.CrossEntropyLoss()\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    optim = paddle.optimizer.Adam(learning_rate=learning_rate, parameters=model.parameters())\n",
    "    lr_scheduler = paddle.optimizer.lr.StepDecay(learning_rate, step_size=step_size, gamma=gamma, last_epoch=- 1, verbose=False)\n",
    "\n",
    "    save_dir = os.path.join('./checkpoint','{}'.format(subdir),'{}-{}-{}'.format(model.full_name(), max_epoches, learning_rate),)\n",
    "    \n",
    "    for epoch in range(max_epoches):\n",
    "        print('**'*10,'Epoch {}/{}'.format(epoch+1,max_epoches),'**'*10)\n",
    "        print('last_lr_rate:{} '.format(history['lr'][-1]))      \n",
    "#         print('-' * 20)\n",
    "        accuracy_recorder = {'train': [], 'val': []}\n",
    "        loss_recorder = {'train': [], 'val': []}\n",
    "        \n",
    "        model.train()\n",
    "        for tr_batch_id, (train_data, train_label) in enumerate(train_loader()) :   \n",
    "            numb = int(train_data.shape[2]/2)\n",
    "            predicts= model(train_data[:,:,:numb], train_data[:,:,numb:]) \n",
    "            loss = loss_fn(predicts, train_label)\n",
    "            acc = paddle.static.accuracy(predicts, train_label)\n",
    "            if (tr_batch_id)%10 == 0:\n",
    "                print(\".......: {}/{} \\tLoss: {} \\tAcc: {}\".format(tr_batch_id+1, \n",
    "                            len(train_loader),\"%.4f\" % loss.numpy(),\"%.3f\" % acc.numpy()))            \n",
    "                accuracy_recorder['train'].append(acc.item())\n",
    "                loss_recorder['train'].append(loss.item())   \n",
    "                        \n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.clear_grad()  \n",
    "        lr_scheduler.step()  \n",
    "        history[\"lr\"].append(lr_scheduler.last_lr)\n",
    "        print(\"     train: \\t loss:[{}], \\t Acc:[{}]\".format(\"%.3f\" % \n",
    "            np.mean(loss_recorder['train']),\"%.4f\" % np.mean(accuracy_recorder['train'])))\n",
    "\n",
    "        model.eval()\n",
    "        for ts_batch_id, (val_data, val_label) in enumerate(val_loader()):   \n",
    "            numb = int(val_data.shape[2]/2)\n",
    "            predicts= model(val_data[:,:,:numb], val_data[:,:,numb:]) \n",
    "            loss = loss_fn(predicts, val_label)\n",
    "            acc = paddle.static.accuracy(predicts, val_label)\n",
    "            \n",
    "            accuracy_recorder['val'].append(acc.item())\n",
    "            loss_recorder['val'].append(loss.item())\n",
    "        print(\"     val:   \\t loss:[{}], \\t Acc:[{}]\".format(\"%.3f\" % \n",
    "              np.mean(loss_recorder['val']),\"%.4f\" % np.mean(accuracy_recorder['val'])))\n",
    "        \n",
    "        \n",
    "        #BEST MODEL\n",
    "        score = np.mean(accuracy_recorder['val'])\n",
    "\n",
    "        if score > history['best_score']['val']:\n",
    "            history['best_score']['val'] = score           \n",
    "            bestacc.append(score)\n",
    "\n",
    "            paddle.save(model.state_dict(), os.path.join(save_dir,'best_model.pdparams'.format(history['best_score'])))\n",
    "\n",
    "            best_model = model.state_dict()\n",
    "            print('<<<Save>>><<<Save>>><<<Save>>>best_Acc:{}'.format(\"%.2f\" %score))\n",
    "        # record loss and accuracy\n",
    "        history[\"loss\"][\"train\"].append(np.mean(loss_recorder[\"train\"]))\n",
    "        history[\"loss\"][\"val\"].append(np.mean(loss_recorder[\"val\"]))\n",
    "        history[\"accuracy\"][\"train\"].append(np.mean(accuracy_recorder[\"train\"]))\n",
    "        history[\"accuracy\"][\"val\"].append(np.mean(accuracy_recorder[\"val\"]))\n",
    "    model.set_state_dict(best_model)\n",
    "\n",
    "    with open(os.path.join(save_dir,'history-{}.pkl'.format(model.full_name())), 'wb+') as f:\n",
    "            pickle.dump(history, f)\n",
    "\n",
    "    print('best score:{}'.format(history['best_score']['val']))\n",
    "    pltm(history,save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
